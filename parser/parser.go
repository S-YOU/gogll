// Package parser is generated by gogll. Do not edit.

package parser

import (
	"bytes"
	"fmt"
	"os"
	"sort"
	"strings"

	"github.com/goccmack/gogll/lexer"
	"github.com/goccmack/gogll/parser/bsr"
	"github.com/goccmack/gogll/parser/slot"
	"github.com/goccmack/gogll/parser/symbols"
	"github.com/goccmack/gogll/token"
)

var (
	cI = 0

	R *descriptors
	U *descriptors

	popped   map[poppedNode]bool
	crf      map[clusterNode][]*crfNode
	crfNodes map[crfNode]*crfNode

	lex         *lexer.Lexer
	parseErrors []*Error
)

func initParser(l *lexer.Lexer) {
	lex = l
	cI = 0
	R, U = &descriptors{}, &descriptors{}
	popped = make(map[poppedNode]bool)
	crf = map[clusterNode][]*crfNode{
		{symbols.NT_GoGLL, 0}: {},
	}
	crfNodes = map[crfNode]*crfNode{}
	bsr.Init(symbols.NT_GoGLL, lex)
	parseErrors = nil
}

func Parse(l *lexer.Lexer) (error, []*Error) {
	initParser(l)
	var L slot.Label
	m, cU := len(l.Tokens)-1, 0
	ntAdd(symbols.NT_GoGLL, 0)
	// DumpDescriptors()
	for !R.empty() {
		L, cU, cI = R.remove()

		// fmt.Println()
		// fmt.Printf("L:%s, cI:%d, I[cI]:%s, cU:%d\n", L, cI, lex.Tokens[cI], cU)
		// DumpDescriptors()

		switch L {
		case slot.GoGLL0R0: // GoGLL : ∙Package Rules

			call(slot.GoGLL0R1, cU, cI)
		case slot.GoGLL0R1: // GoGLL : Package ∙Rules

			if !testSelect(slot.GoGLL0R1) {
				parseError(slot.GoGLL0R1, cI, first[slot.GoGLL0R1])
				break
			}

			call(slot.GoGLL0R2, cU, cI)
		case slot.GoGLL0R2: // GoGLL : Package Rules ∙

			if follow(symbols.NT_GoGLL) {
				rtn(symbols.NT_GoGLL, cU, cI)
			} else {
				parseError(slot.GoGLL0R0, cI, followSets[symbols.NT_GoGLL])
			}
		case slot.LexAlternates0R0: // LexAlternates : ∙RegExp

			call(slot.LexAlternates0R1, cU, cI)
		case slot.LexAlternates0R1: // LexAlternates : RegExp ∙

			if follow(symbols.NT_LexAlternates) {
				rtn(symbols.NT_LexAlternates, cU, cI)
			} else {
				parseError(slot.LexAlternates0R0, cI, followSets[symbols.NT_LexAlternates])
			}
		case slot.LexAlternates1R0: // LexAlternates : ∙RegExp | LexAlternates

			call(slot.LexAlternates1R1, cU, cI)
		case slot.LexAlternates1R1: // LexAlternates : RegExp ∙| LexAlternates

			if !testSelect(slot.LexAlternates1R1) {
				parseError(slot.LexAlternates1R1, cI, first[slot.LexAlternates1R1])
				break
			}

			bsr.Add(slot.LexAlternates1R2, cU, cI, cI+1)
			cI++
			if !testSelect(slot.LexAlternates1R2) {
				parseError(slot.LexAlternates1R2, cI, first[slot.LexAlternates1R2])
				break
			}

			call(slot.LexAlternates1R3, cU, cI)
		case slot.LexAlternates1R3: // LexAlternates : RegExp | LexAlternates ∙

			if follow(symbols.NT_LexAlternates) {
				rtn(symbols.NT_LexAlternates, cU, cI)
			} else {
				parseError(slot.LexAlternates1R0, cI, followSets[symbols.NT_LexAlternates])
			}
		case slot.LexBracket0R0: // LexBracket : ∙LexGroup

			call(slot.LexBracket0R1, cU, cI)
		case slot.LexBracket0R1: // LexBracket : LexGroup ∙

			if follow(symbols.NT_LexBracket) {
				rtn(symbols.NT_LexBracket, cU, cI)
			} else {
				parseError(slot.LexBracket0R0, cI, followSets[symbols.NT_LexBracket])
			}
		case slot.LexBracket1R0: // LexBracket : ∙LexOptional

			call(slot.LexBracket1R1, cU, cI)
		case slot.LexBracket1R1: // LexBracket : LexOptional ∙

			if follow(symbols.NT_LexBracket) {
				rtn(symbols.NT_LexBracket, cU, cI)
			} else {
				parseError(slot.LexBracket1R0, cI, followSets[symbols.NT_LexBracket])
			}
		case slot.LexBracket2R0: // LexBracket : ∙LexZeroOrMore

			call(slot.LexBracket2R1, cU, cI)
		case slot.LexBracket2R1: // LexBracket : LexZeroOrMore ∙

			if follow(symbols.NT_LexBracket) {
				rtn(symbols.NT_LexBracket, cU, cI)
			} else {
				parseError(slot.LexBracket2R0, cI, followSets[symbols.NT_LexBracket])
			}
		case slot.LexBracket3R0: // LexBracket : ∙LexOneOrMore

			call(slot.LexBracket3R1, cU, cI)
		case slot.LexBracket3R1: // LexBracket : LexOneOrMore ∙

			if follow(symbols.NT_LexBracket) {
				rtn(symbols.NT_LexBracket, cU, cI)
			} else {
				parseError(slot.LexBracket3R0, cI, followSets[symbols.NT_LexBracket])
			}
		case slot.LexGroup0R0: // LexGroup : ∙( LexAlternates )

			bsr.Add(slot.LexGroup0R1, cU, cI, cI+1)
			cI++
			if !testSelect(slot.LexGroup0R1) {
				parseError(slot.LexGroup0R1, cI, first[slot.LexGroup0R1])
				break
			}

			call(slot.LexGroup0R2, cU, cI)
		case slot.LexGroup0R2: // LexGroup : ( LexAlternates ∙)

			if !testSelect(slot.LexGroup0R2) {
				parseError(slot.LexGroup0R2, cI, first[slot.LexGroup0R2])
				break
			}

			bsr.Add(slot.LexGroup0R3, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_LexGroup) {
				rtn(symbols.NT_LexGroup, cU, cI)
			} else {
				parseError(slot.LexGroup0R0, cI, followSets[symbols.NT_LexGroup])
			}
		case slot.LexOneOrMore0R0: // LexOneOrMore : ∙< LexAlternates >

			bsr.Add(slot.LexOneOrMore0R1, cU, cI, cI+1)
			cI++
			if !testSelect(slot.LexOneOrMore0R1) {
				parseError(slot.LexOneOrMore0R1, cI, first[slot.LexOneOrMore0R1])
				break
			}

			call(slot.LexOneOrMore0R2, cU, cI)
		case slot.LexOneOrMore0R2: // LexOneOrMore : < LexAlternates ∙>

			if !testSelect(slot.LexOneOrMore0R2) {
				parseError(slot.LexOneOrMore0R2, cI, first[slot.LexOneOrMore0R2])
				break
			}

			bsr.Add(slot.LexOneOrMore0R3, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_LexOneOrMore) {
				rtn(symbols.NT_LexOneOrMore, cU, cI)
			} else {
				parseError(slot.LexOneOrMore0R0, cI, followSets[symbols.NT_LexOneOrMore])
			}
		case slot.LexOptional0R0: // LexOptional : ∙[ LexAlternates ]

			bsr.Add(slot.LexOptional0R1, cU, cI, cI+1)
			cI++
			if !testSelect(slot.LexOptional0R1) {
				parseError(slot.LexOptional0R1, cI, first[slot.LexOptional0R1])
				break
			}

			call(slot.LexOptional0R2, cU, cI)
		case slot.LexOptional0R2: // LexOptional : [ LexAlternates ∙]

			if !testSelect(slot.LexOptional0R2) {
				parseError(slot.LexOptional0R2, cI, first[slot.LexOptional0R2])
				break
			}

			bsr.Add(slot.LexOptional0R3, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_LexOptional) {
				rtn(symbols.NT_LexOptional, cU, cI)
			} else {
				parseError(slot.LexOptional0R0, cI, followSets[symbols.NT_LexOptional])
			}
		case slot.LexRule0R0: // LexRule : ∙TokID : RegExp ;

			call(slot.LexRule0R1, cU, cI)
		case slot.LexRule0R1: // LexRule : TokID ∙: RegExp ;

			if !testSelect(slot.LexRule0R1) {
				parseError(slot.LexRule0R1, cI, first[slot.LexRule0R1])
				break
			}

			bsr.Add(slot.LexRule0R2, cU, cI, cI+1)
			cI++
			if !testSelect(slot.LexRule0R2) {
				parseError(slot.LexRule0R2, cI, first[slot.LexRule0R2])
				break
			}

			call(slot.LexRule0R3, cU, cI)
		case slot.LexRule0R3: // LexRule : TokID : RegExp ∙;

			if !testSelect(slot.LexRule0R3) {
				parseError(slot.LexRule0R3, cI, first[slot.LexRule0R3])
				break
			}

			bsr.Add(slot.LexRule0R4, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_LexRule) {
				rtn(symbols.NT_LexRule, cU, cI)
			} else {
				parseError(slot.LexRule0R0, cI, followSets[symbols.NT_LexRule])
			}
		case slot.LexSymbol0R0: // LexSymbol : ∙.

			bsr.Add(slot.LexSymbol0R1, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_LexSymbol) {
				rtn(symbols.NT_LexSymbol, cU, cI)
			} else {
				parseError(slot.LexSymbol0R0, cI, followSets[symbols.NT_LexSymbol])
			}
		case slot.LexSymbol1R0: // LexSymbol : ∙any string_lit

			bsr.Add(slot.LexSymbol1R1, cU, cI, cI+1)
			cI++
			if !testSelect(slot.LexSymbol1R1) {
				parseError(slot.LexSymbol1R1, cI, first[slot.LexSymbol1R1])
				break
			}

			bsr.Add(slot.LexSymbol1R2, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_LexSymbol) {
				rtn(symbols.NT_LexSymbol, cU, cI)
			} else {
				parseError(slot.LexSymbol1R0, cI, followSets[symbols.NT_LexSymbol])
			}
		case slot.LexSymbol2R0: // LexSymbol : ∙char_lit

			bsr.Add(slot.LexSymbol2R1, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_LexSymbol) {
				rtn(symbols.NT_LexSymbol, cU, cI)
			} else {
				parseError(slot.LexSymbol2R0, cI, followSets[symbols.NT_LexSymbol])
			}
		case slot.LexSymbol3R0: // LexSymbol : ∙LexBracket

			call(slot.LexSymbol3R1, cU, cI)
		case slot.LexSymbol3R1: // LexSymbol : LexBracket ∙

			if follow(symbols.NT_LexSymbol) {
				rtn(symbols.NT_LexSymbol, cU, cI)
			} else {
				parseError(slot.LexSymbol3R0, cI, followSets[symbols.NT_LexSymbol])
			}
		case slot.LexSymbol4R0: // LexSymbol : ∙not string_lit

			bsr.Add(slot.LexSymbol4R1, cU, cI, cI+1)
			cI++
			if !testSelect(slot.LexSymbol4R1) {
				parseError(slot.LexSymbol4R1, cI, first[slot.LexSymbol4R1])
				break
			}

			bsr.Add(slot.LexSymbol4R2, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_LexSymbol) {
				rtn(symbols.NT_LexSymbol, cU, cI)
			} else {
				parseError(slot.LexSymbol4R0, cI, followSets[symbols.NT_LexSymbol])
			}
		case slot.LexSymbol5R0: // LexSymbol : ∙UnicodeClass

			call(slot.LexSymbol5R1, cU, cI)
		case slot.LexSymbol5R1: // LexSymbol : UnicodeClass ∙

			if follow(symbols.NT_LexSymbol) {
				rtn(symbols.NT_LexSymbol, cU, cI)
			} else {
				parseError(slot.LexSymbol5R0, cI, followSets[symbols.NT_LexSymbol])
			}
		case slot.LexZeroOrMore0R0: // LexZeroOrMore : ∙{ LexAlternates }

			bsr.Add(slot.LexZeroOrMore0R1, cU, cI, cI+1)
			cI++
			if !testSelect(slot.LexZeroOrMore0R1) {
				parseError(slot.LexZeroOrMore0R1, cI, first[slot.LexZeroOrMore0R1])
				break
			}

			call(slot.LexZeroOrMore0R2, cU, cI)
		case slot.LexZeroOrMore0R2: // LexZeroOrMore : { LexAlternates ∙}

			if !testSelect(slot.LexZeroOrMore0R2) {
				parseError(slot.LexZeroOrMore0R2, cI, first[slot.LexZeroOrMore0R2])
				break
			}

			bsr.Add(slot.LexZeroOrMore0R3, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_LexZeroOrMore) {
				rtn(symbols.NT_LexZeroOrMore, cU, cI)
			} else {
				parseError(slot.LexZeroOrMore0R0, cI, followSets[symbols.NT_LexZeroOrMore])
			}
		case slot.NT0R0: // NT : ∙nt

			bsr.Add(slot.NT0R1, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_NT) {
				rtn(symbols.NT_NT, cU, cI)
			} else {
				parseError(slot.NT0R0, cI, followSets[symbols.NT_NT])
			}
		case slot.Package0R0: // Package : ∙package string_lit

			bsr.Add(slot.Package0R1, cU, cI, cI+1)
			cI++
			if !testSelect(slot.Package0R1) {
				parseError(slot.Package0R1, cI, first[slot.Package0R1])
				break
			}

			bsr.Add(slot.Package0R2, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_Package) {
				rtn(symbols.NT_Package, cU, cI)
			} else {
				parseError(slot.Package0R0, cI, followSets[symbols.NT_Package])
			}
		case slot.RegExp0R0: // RegExp : ∙LexSymbol

			call(slot.RegExp0R1, cU, cI)
		case slot.RegExp0R1: // RegExp : LexSymbol ∙

			if follow(symbols.NT_RegExp) {
				rtn(symbols.NT_RegExp, cU, cI)
			} else {
				parseError(slot.RegExp0R0, cI, followSets[symbols.NT_RegExp])
			}
		case slot.RegExp1R0: // RegExp : ∙LexSymbol RegExp

			call(slot.RegExp1R1, cU, cI)
		case slot.RegExp1R1: // RegExp : LexSymbol ∙RegExp

			if !testSelect(slot.RegExp1R1) {
				parseError(slot.RegExp1R1, cI, first[slot.RegExp1R1])
				break
			}

			call(slot.RegExp1R2, cU, cI)
		case slot.RegExp1R2: // RegExp : LexSymbol RegExp ∙

			if follow(symbols.NT_RegExp) {
				rtn(symbols.NT_RegExp, cU, cI)
			} else {
				parseError(slot.RegExp1R0, cI, followSets[symbols.NT_RegExp])
			}
		case slot.Rule0R0: // Rule : ∙LexRule

			call(slot.Rule0R1, cU, cI)
		case slot.Rule0R1: // Rule : LexRule ∙

			if follow(symbols.NT_Rule) {
				rtn(symbols.NT_Rule, cU, cI)
			} else {
				parseError(slot.Rule0R0, cI, followSets[symbols.NT_Rule])
			}
		case slot.Rule1R0: // Rule : ∙SyntaxRule

			call(slot.Rule1R1, cU, cI)
		case slot.Rule1R1: // Rule : SyntaxRule ∙

			if follow(symbols.NT_Rule) {
				rtn(symbols.NT_Rule, cU, cI)
			} else {
				parseError(slot.Rule1R0, cI, followSets[symbols.NT_Rule])
			}
		case slot.Rules0R0: // Rules : ∙Rule

			call(slot.Rules0R1, cU, cI)
		case slot.Rules0R1: // Rules : Rule ∙

			if follow(symbols.NT_Rules) {
				rtn(symbols.NT_Rules, cU, cI)
			} else {
				parseError(slot.Rules0R0, cI, followSets[symbols.NT_Rules])
			}
		case slot.Rules1R0: // Rules : ∙Rule Rules

			call(slot.Rules1R1, cU, cI)
		case slot.Rules1R1: // Rules : Rule ∙Rules

			if !testSelect(slot.Rules1R1) {
				parseError(slot.Rules1R1, cI, first[slot.Rules1R1])
				break
			}

			call(slot.Rules1R2, cU, cI)
		case slot.Rules1R2: // Rules : Rule Rules ∙

			if follow(symbols.NT_Rules) {
				rtn(symbols.NT_Rules, cU, cI)
			} else {
				parseError(slot.Rules1R0, cI, followSets[symbols.NT_Rules])
			}
		case slot.SyntaxAlternate0R0: // SyntaxAlternate : ∙SyntaxSymbols

			call(slot.SyntaxAlternate0R1, cU, cI)
		case slot.SyntaxAlternate0R1: // SyntaxAlternate : SyntaxSymbols ∙

			if follow(symbols.NT_SyntaxAlternate) {
				rtn(symbols.NT_SyntaxAlternate, cU, cI)
			} else {
				parseError(slot.SyntaxAlternate0R0, cI, followSets[symbols.NT_SyntaxAlternate])
			}
		case slot.SyntaxAlternate1R0: // SyntaxAlternate : ∙empty

			bsr.Add(slot.SyntaxAlternate1R1, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_SyntaxAlternate) {
				rtn(symbols.NT_SyntaxAlternate, cU, cI)
			} else {
				parseError(slot.SyntaxAlternate1R0, cI, followSets[symbols.NT_SyntaxAlternate])
			}
		case slot.SyntaxAlternates0R0: // SyntaxAlternates : ∙SyntaxAlternate

			call(slot.SyntaxAlternates0R1, cU, cI)
		case slot.SyntaxAlternates0R1: // SyntaxAlternates : SyntaxAlternate ∙

			if follow(symbols.NT_SyntaxAlternates) {
				rtn(symbols.NT_SyntaxAlternates, cU, cI)
			} else {
				parseError(slot.SyntaxAlternates0R0, cI, followSets[symbols.NT_SyntaxAlternates])
			}
		case slot.SyntaxAlternates1R0: // SyntaxAlternates : ∙SyntaxAlternate | SyntaxAlternates

			call(slot.SyntaxAlternates1R1, cU, cI)
		case slot.SyntaxAlternates1R1: // SyntaxAlternates : SyntaxAlternate ∙| SyntaxAlternates

			if !testSelect(slot.SyntaxAlternates1R1) {
				parseError(slot.SyntaxAlternates1R1, cI, first[slot.SyntaxAlternates1R1])
				break
			}

			bsr.Add(slot.SyntaxAlternates1R2, cU, cI, cI+1)
			cI++
			if !testSelect(slot.SyntaxAlternates1R2) {
				parseError(slot.SyntaxAlternates1R2, cI, first[slot.SyntaxAlternates1R2])
				break
			}

			call(slot.SyntaxAlternates1R3, cU, cI)
		case slot.SyntaxAlternates1R3: // SyntaxAlternates : SyntaxAlternate | SyntaxAlternates ∙

			if follow(symbols.NT_SyntaxAlternates) {
				rtn(symbols.NT_SyntaxAlternates, cU, cI)
			} else {
				parseError(slot.SyntaxAlternates1R0, cI, followSets[symbols.NT_SyntaxAlternates])
			}
		case slot.SyntaxRule0R0: // SyntaxRule : ∙NT : SyntaxAlternates ;

			call(slot.SyntaxRule0R1, cU, cI)
		case slot.SyntaxRule0R1: // SyntaxRule : NT ∙: SyntaxAlternates ;

			if !testSelect(slot.SyntaxRule0R1) {
				parseError(slot.SyntaxRule0R1, cI, first[slot.SyntaxRule0R1])
				break
			}

			bsr.Add(slot.SyntaxRule0R2, cU, cI, cI+1)
			cI++
			if !testSelect(slot.SyntaxRule0R2) {
				parseError(slot.SyntaxRule0R2, cI, first[slot.SyntaxRule0R2])
				break
			}

			call(slot.SyntaxRule0R3, cU, cI)
		case slot.SyntaxRule0R3: // SyntaxRule : NT : SyntaxAlternates ∙;

			if !testSelect(slot.SyntaxRule0R3) {
				parseError(slot.SyntaxRule0R3, cI, first[slot.SyntaxRule0R3])
				break
			}

			bsr.Add(slot.SyntaxRule0R4, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_SyntaxRule) {
				rtn(symbols.NT_SyntaxRule, cU, cI)
			} else {
				parseError(slot.SyntaxRule0R0, cI, followSets[symbols.NT_SyntaxRule])
			}
		case slot.SyntaxSymbol0R0: // SyntaxSymbol : ∙NT

			call(slot.SyntaxSymbol0R1, cU, cI)
		case slot.SyntaxSymbol0R1: // SyntaxSymbol : NT ∙

			if follow(symbols.NT_SyntaxSymbol) {
				rtn(symbols.NT_SyntaxSymbol, cU, cI)
			} else {
				parseError(slot.SyntaxSymbol0R0, cI, followSets[symbols.NT_SyntaxSymbol])
			}
		case slot.SyntaxSymbol1R0: // SyntaxSymbol : ∙TokID

			call(slot.SyntaxSymbol1R1, cU, cI)
		case slot.SyntaxSymbol1R1: // SyntaxSymbol : TokID ∙

			if follow(symbols.NT_SyntaxSymbol) {
				rtn(symbols.NT_SyntaxSymbol, cU, cI)
			} else {
				parseError(slot.SyntaxSymbol1R0, cI, followSets[symbols.NT_SyntaxSymbol])
			}
		case slot.SyntaxSymbol2R0: // SyntaxSymbol : ∙string_lit

			bsr.Add(slot.SyntaxSymbol2R1, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_SyntaxSymbol) {
				rtn(symbols.NT_SyntaxSymbol, cU, cI)
			} else {
				parseError(slot.SyntaxSymbol2R0, cI, followSets[symbols.NT_SyntaxSymbol])
			}
		case slot.SyntaxSymbols0R0: // SyntaxSymbols : ∙SyntaxSymbol

			call(slot.SyntaxSymbols0R1, cU, cI)
		case slot.SyntaxSymbols0R1: // SyntaxSymbols : SyntaxSymbol ∙

			if follow(symbols.NT_SyntaxSymbols) {
				rtn(symbols.NT_SyntaxSymbols, cU, cI)
			} else {
				parseError(slot.SyntaxSymbols0R0, cI, followSets[symbols.NT_SyntaxSymbols])
			}
		case slot.SyntaxSymbols1R0: // SyntaxSymbols : ∙SyntaxSymbol SyntaxSymbols

			call(slot.SyntaxSymbols1R1, cU, cI)
		case slot.SyntaxSymbols1R1: // SyntaxSymbols : SyntaxSymbol ∙SyntaxSymbols

			if !testSelect(slot.SyntaxSymbols1R1) {
				parseError(slot.SyntaxSymbols1R1, cI, first[slot.SyntaxSymbols1R1])
				break
			}

			call(slot.SyntaxSymbols1R2, cU, cI)
		case slot.SyntaxSymbols1R2: // SyntaxSymbols : SyntaxSymbol SyntaxSymbols ∙

			if follow(symbols.NT_SyntaxSymbols) {
				rtn(symbols.NT_SyntaxSymbols, cU, cI)
			} else {
				parseError(slot.SyntaxSymbols1R0, cI, followSets[symbols.NT_SyntaxSymbols])
			}
		case slot.TokID0R0: // TokID : ∙tokid

			bsr.Add(slot.TokID0R1, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_TokID) {
				rtn(symbols.NT_TokID, cU, cI)
			} else {
				parseError(slot.TokID0R0, cI, followSets[symbols.NT_TokID])
			}
		case slot.UnicodeClass0R0: // UnicodeClass : ∙letter

			bsr.Add(slot.UnicodeClass0R1, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_UnicodeClass) {
				rtn(symbols.NT_UnicodeClass, cU, cI)
			} else {
				parseError(slot.UnicodeClass0R0, cI, followSets[symbols.NT_UnicodeClass])
			}
		case slot.UnicodeClass1R0: // UnicodeClass : ∙upcase

			bsr.Add(slot.UnicodeClass1R1, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_UnicodeClass) {
				rtn(symbols.NT_UnicodeClass, cU, cI)
			} else {
				parseError(slot.UnicodeClass1R0, cI, followSets[symbols.NT_UnicodeClass])
			}
		case slot.UnicodeClass2R0: // UnicodeClass : ∙lowcase

			bsr.Add(slot.UnicodeClass2R1, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_UnicodeClass) {
				rtn(symbols.NT_UnicodeClass, cU, cI)
			} else {
				parseError(slot.UnicodeClass2R0, cI, followSets[symbols.NT_UnicodeClass])
			}
		case slot.UnicodeClass3R0: // UnicodeClass : ∙number

			bsr.Add(slot.UnicodeClass3R1, cU, cI, cI+1)
			cI++
			if follow(symbols.NT_UnicodeClass) {
				rtn(symbols.NT_UnicodeClass, cU, cI)
			} else {
				parseError(slot.UnicodeClass3R0, cI, followSets[symbols.NT_UnicodeClass])
			}

		default:
			panic("This must not happen")
		}
	}
	if !bsr.Contain(symbols.NT_GoGLL, 0, m) {
		sortParseErrors()
		err := fmt.Errorf("Error: Parse Failed right extent=%d, m=%d",
			bsr.GetRightExtent(), len(l.Tokens))
		return err, parseErrors
	}
	return nil, nil
}

func ntAdd(nt symbols.NT, j int) {
	// fmt.Printf("ntAdd(%s, %d)\n", nt, j)
	failed := true
	expected := map[token.Type]string{}
	for _, l := range slot.GetAlternates(nt) {
		if testSelect(l) {
			dscAdd(l, j, j)
			failed = false
		} else {
			for k, v := range first[l] {
				expected[k] = v
			}
		}
	}
	if failed {
		for _, l := range slot.GetAlternates(nt) {
			parseError(l, j, expected)
		}
	}
}

/*** Call Return Forest ***/

type poppedNode struct {
	X    symbols.NT
	k, j int
}

type clusterNode struct {
	X symbols.NT
	k int
}

type crfNode struct {
	L slot.Label
	i int
}

/*
suppose that L is Y ::=αX ·β
if there is no CRF node labelled (L,i)
	create one let u be the CRF node labelled (L,i)
if there is no CRF node labelled (X, j) {
	create a CRF node v labelled (X, j)
	create an edge from v to u
	ntAdd(X, j)
} else {
	let v be the CRF node labelled (X, j)
	if there is not an edge from v to u {
		create an edge from v to u
		for all ((X, j,h)∈P) {
			dscAdd(L, i, h);
			bsrAdd(L, i, j, h)
		}
	}
}
*/
func call(L slot.Label, i, j int) {
	// fmt.Printf("call(%s,%d,%d)\n", L,i,j)
	u, exist := crfNodes[crfNode{L, i}]
	// fmt.Printf("  u exist=%t\n", exist)
	if !exist {
		u = &crfNode{L, i}
		crfNodes[*u] = u
	}
	X := L.Symbols()[L.Pos()-1].(symbols.NT)
	ndV := clusterNode{X, j}
	v, exist := crf[ndV]
	if !exist {
		// fmt.Println("  v !exist")
		crf[ndV] = []*crfNode{u}
		ntAdd(X, j)
	} else {
		// fmt.Println("  v exist")
		if !existEdge(v, u) {
			// fmt.Printf("  !existEdge(%v)\n", u)
			crf[ndV] = append(v, u)
			// fmt.Printf("|popped|=%d\n", len(popped))
			for pnd, _ := range popped {
				if pnd.X == X && pnd.k == j {
					dscAdd(L, i, pnd.j)
					bsr.Add(L, i, j, pnd.j)
				}
			}
		}
	}
}

func existEdge(nds []*crfNode, nd *crfNode) bool {
	for _, nd1 := range nds {
		if nd1 == nd {
			return true
		}
	}
	return false
}

func rtn(X symbols.NT, k, j int) {
	// fmt.Printf("rtn(%s,%d,%d)\n", X,k,j)
	p := poppedNode{X, k, j}
	if _, exist := popped[p]; !exist {
		popped[p] = true
		for _, nd := range crf[clusterNode{X, k}] {
			dscAdd(nd.L, nd.i, j)
			bsr.Add(nd.L, nd.i, k, j)
		}
	}
}

func CRFString() string {
	buf := new(bytes.Buffer)
	buf.WriteString("CRF: {")
	for cn, nds := range crf {
		for _, nd := range nds {
			fmt.Fprintf(buf, "%s->%s, ", cn, nd)
		}
	}
	buf.WriteString("}")
	return buf.String()
}

func (cn clusterNode) String() string {
	return fmt.Sprintf("(%s,%d)", cn.X, cn.k)
}

func (n crfNode) String() string {
	return fmt.Sprintf("(%s,%d)", n.L.String(), n.i)
}

func PoppedString() string {
	buf := new(bytes.Buffer)
	buf.WriteString("Popped: {")
	for p, _ := range popped {
		fmt.Fprintf(buf, "(%s,%d,%d) ", p.X, p.k, p.j)
	}
	buf.WriteString("}")
	return buf.String()
}

/*** descriptors ***/

type descriptors struct {
	set []*descriptor
}

func (ds *descriptors) contain(d *descriptor) bool {
	for _, d1 := range ds.set {
		if d1 == d {
			return true
		}
	}
	return false
}

func (ds *descriptors) empty() bool {
	return len(ds.set) == 0
}

func (ds *descriptors) String() string {
	buf := new(bytes.Buffer)
	buf.WriteString("{")
	for i, d := range ds.set {
		if i > 0 {
			buf.WriteString("; ")
		}
		fmt.Fprintf(buf, "%s", d)
	}
	buf.WriteString("}")
	return buf.String()
}

type descriptor struct {
	L slot.Label
	k int
	i int
}

func (d *descriptor) String() string {
	return fmt.Sprintf("%s,%d,%d", d.L, d.k, d.i)
}

func dscAdd(L slot.Label, k, i int) {
	// fmt.Printf("dscAdd(%s,%d,%d)\n", L, k, i)
	d := &descriptor{L, k, i}
	if !U.contain(d) {
		R.set = append(R.set, d)
		U.set = append(U.set, d)
	}
}

func (ds *descriptors) remove() (L slot.Label, k, i int) {
	d := ds.set[len(ds.set)-1]
	ds.set = ds.set[:len(ds.set)-1]
	// fmt.Printf("remove: %s,%d,%d\n", d.L, d.k, d.i)
	return d.L, d.k, d.i
}

func DumpDescriptors() {
	DumpR()
	DumpU()
}

func DumpR() {
	fmt.Println("R:")
	for _, d := range R.set {
		fmt.Printf(" %s\n", d)
	}
}

func DumpU() {
	fmt.Println("U:")
	for _, d := range U.set {
		fmt.Printf(" %s\n", d)
	}
}

/*** TestSelect ***/

func follow(nt symbols.NT) bool {
	_, exist := followSets[nt][lex.Tokens[cI].Type]
	return exist
}

func testSelect(l slot.Label) bool {
	_, exist := first[l][lex.Tokens[cI].Type]
	// fmt.Printf("testSelect(%s) = %t\n", l, exist)
	return exist
}

var first = []map[token.Type]string{
	// GoGLL : ∙Package Rules
	map[token.Type]string{
		token.Type21: "package",
	},
	// GoGLL : Package ∙Rules
	map[token.Type]string{
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// GoGLL : Package Rules ∙
	map[token.Type]string{
		token.EOF: "EOF",
	},
	// LexAlternates : ∙RegExp
	map[token.Type]string{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexAlternates : RegExp ∙
	map[token.Type]string{
		token.Type2:  ")",
		token.Type7:  ">",
		token.Type11: "]",
		token.Type27: "}",
	},
	// LexAlternates : ∙RegExp | LexAlternates
	map[token.Type]string{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexAlternates : RegExp ∙| LexAlternates
	map[token.Type]string{
		token.Type26: "|",
	},
	// LexAlternates : RegExp | ∙LexAlternates
	map[token.Type]string{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexAlternates : RegExp | LexAlternates ∙
	map[token.Type]string{
		token.Type2:  ")",
		token.Type7:  ">",
		token.Type11: "]",
		token.Type27: "}",
	},
	// LexBracket : ∙LexGroup
	map[token.Type]string{
		token.Type1: "(",
	},
	// LexBracket : LexGroup ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexBracket : ∙LexOptional
	map[token.Type]string{
		token.Type8: "[",
	},
	// LexBracket : LexOptional ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexBracket : ∙LexZeroOrMore
	map[token.Type]string{
		token.Type25: "{",
	},
	// LexBracket : LexZeroOrMore ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexBracket : ∙LexOneOrMore
	map[token.Type]string{
		token.Type6: "<",
	},
	// LexBracket : LexOneOrMore ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexGroup : ∙( LexAlternates )
	map[token.Type]string{
		token.Type1: "(",
	},
	// LexGroup : ( ∙LexAlternates )
	map[token.Type]string{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexGroup : ( LexAlternates ∙)
	map[token.Type]string{
		token.Type2: ")",
	},
	// LexGroup : ( LexAlternates ) ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexOneOrMore : ∙< LexAlternates >
	map[token.Type]string{
		token.Type6: "<",
	},
	// LexOneOrMore : < ∙LexAlternates >
	map[token.Type]string{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexOneOrMore : < LexAlternates ∙>
	map[token.Type]string{
		token.Type7: ">",
	},
	// LexOneOrMore : < LexAlternates > ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexOptional : ∙[ LexAlternates ]
	map[token.Type]string{
		token.Type8: "[",
	},
	// LexOptional : [ ∙LexAlternates ]
	map[token.Type]string{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexOptional : [ LexAlternates ∙]
	map[token.Type]string{
		token.Type11: "]",
	},
	// LexOptional : [ LexAlternates ] ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexRule : ∙TokID : RegExp ;
	map[token.Type]string{
		token.Type23: "tokid",
	},
	// LexRule : TokID ∙: RegExp ;
	map[token.Type]string{
		token.Type4: ":",
	},
	// LexRule : TokID : ∙RegExp ;
	map[token.Type]string{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexRule : TokID : RegExp ∙;
	map[token.Type]string{
		token.Type5: ";",
	},
	// LexRule : TokID : RegExp ; ∙
	map[token.Type]string{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// LexSymbol : ∙.
	map[token.Type]string{
		token.Type3: ".",
	},
	// LexSymbol : . ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexSymbol : ∙any string_lit
	map[token.Type]string{
		token.Type13: "any",
	},
	// LexSymbol : any ∙string_lit
	map[token.Type]string{
		token.Type22: "string_lit",
	},
	// LexSymbol : any string_lit ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexSymbol : ∙char_lit
	map[token.Type]string{
		token.Type14: "char_lit",
	},
	// LexSymbol : char_lit ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexSymbol : ∙LexBracket
	map[token.Type]string{
		token.Type1:  "(",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type25: "{",
	},
	// LexSymbol : LexBracket ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexSymbol : ∙not string_lit
	map[token.Type]string{
		token.Type18: "not",
	},
	// LexSymbol : not ∙string_lit
	map[token.Type]string{
		token.Type22: "string_lit",
	},
	// LexSymbol : not string_lit ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexSymbol : ∙UnicodeClass
	map[token.Type]string{
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type20: "number",
		token.Type24: "upcase",
	},
	// LexSymbol : UnicodeClass ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexZeroOrMore : ∙{ LexAlternates }
	map[token.Type]string{
		token.Type25: "{",
	},
	// LexZeroOrMore : { ∙LexAlternates }
	map[token.Type]string{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// LexZeroOrMore : { LexAlternates ∙}
	map[token.Type]string{
		token.Type27: "}",
	},
	// LexZeroOrMore : { LexAlternates } ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// NT : ∙nt
	map[token.Type]string{
		token.Type19: "nt",
	},
	// NT : nt ∙
	map[token.Type]string{
		token.Type4:  ":",
		token.Type5:  ";",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
		token.Type26: "|",
	},
	// Package : ∙package string_lit
	map[token.Type]string{
		token.Type21: "package",
	},
	// Package : package ∙string_lit
	map[token.Type]string{
		token.Type22: "string_lit",
	},
	// Package : package string_lit ∙
	map[token.Type]string{
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// RegExp : ∙LexSymbol
	map[token.Type]string{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// RegExp : LexSymbol ∙
	map[token.Type]string{
		token.Type2:  ")",
		token.Type5:  ";",
		token.Type7:  ">",
		token.Type11: "]",
		token.Type26: "|",
		token.Type27: "}",
	},
	// RegExp : ∙LexSymbol RegExp
	map[token.Type]string{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// RegExp : LexSymbol ∙RegExp
	map[token.Type]string{
		token.Type1:  "(",
		token.Type3:  ".",
		token.Type6:  "<",
		token.Type8:  "[",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
	},
	// RegExp : LexSymbol RegExp ∙
	map[token.Type]string{
		token.Type2:  ")",
		token.Type5:  ";",
		token.Type7:  ">",
		token.Type11: "]",
		token.Type26: "|",
		token.Type27: "}",
	},
	// Rule : ∙LexRule
	map[token.Type]string{
		token.Type23: "tokid",
	},
	// Rule : LexRule ∙
	map[token.Type]string{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// Rule : ∙SyntaxRule
	map[token.Type]string{
		token.Type19: "nt",
	},
	// Rule : SyntaxRule ∙
	map[token.Type]string{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// Rules : ∙Rule
	map[token.Type]string{
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// Rules : Rule ∙
	map[token.Type]string{
		token.EOF: "EOF",
	},
	// Rules : ∙Rule Rules
	map[token.Type]string{
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// Rules : Rule ∙Rules
	map[token.Type]string{
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// Rules : Rule Rules ∙
	map[token.Type]string{
		token.EOF: "EOF",
	},
	// SyntaxAlternate : ∙SyntaxSymbols
	map[token.Type]string{
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxAlternate : SyntaxSymbols ∙
	map[token.Type]string{
		token.Type5:  ";",
		token.Type26: "|",
	},
	// SyntaxAlternate : ∙empty
	map[token.Type]string{
		token.Type15: "empty",
	},
	// SyntaxAlternate : empty ∙
	map[token.Type]string{
		token.Type5:  ";",
		token.Type26: "|",
	},
	// SyntaxAlternates : ∙SyntaxAlternate
	map[token.Type]string{
		token.Type15: "empty",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxAlternates : SyntaxAlternate ∙
	map[token.Type]string{
		token.Type5: ";",
	},
	// SyntaxAlternates : ∙SyntaxAlternate | SyntaxAlternates
	map[token.Type]string{
		token.Type15: "empty",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxAlternates : SyntaxAlternate ∙| SyntaxAlternates
	map[token.Type]string{
		token.Type26: "|",
	},
	// SyntaxAlternates : SyntaxAlternate | ∙SyntaxAlternates
	map[token.Type]string{
		token.Type15: "empty",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxAlternates : SyntaxAlternate | SyntaxAlternates ∙
	map[token.Type]string{
		token.Type5: ";",
	},
	// SyntaxRule : ∙NT : SyntaxAlternates ;
	map[token.Type]string{
		token.Type19: "nt",
	},
	// SyntaxRule : NT ∙: SyntaxAlternates ;
	map[token.Type]string{
		token.Type4: ":",
	},
	// SyntaxRule : NT : ∙SyntaxAlternates ;
	map[token.Type]string{
		token.Type15: "empty",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxRule : NT : SyntaxAlternates ∙;
	map[token.Type]string{
		token.Type5: ";",
	},
	// SyntaxRule : NT : SyntaxAlternates ; ∙
	map[token.Type]string{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// SyntaxSymbol : ∙NT
	map[token.Type]string{
		token.Type19: "nt",
	},
	// SyntaxSymbol : NT ∙
	map[token.Type]string{
		token.Type5:  ";",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
		token.Type26: "|",
	},
	// SyntaxSymbol : ∙TokID
	map[token.Type]string{
		token.Type23: "tokid",
	},
	// SyntaxSymbol : TokID ∙
	map[token.Type]string{
		token.Type5:  ";",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
		token.Type26: "|",
	},
	// SyntaxSymbol : ∙string_lit
	map[token.Type]string{
		token.Type22: "string_lit",
	},
	// SyntaxSymbol : string_lit ∙
	map[token.Type]string{
		token.Type5:  ";",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
		token.Type26: "|",
	},
	// SyntaxSymbols : ∙SyntaxSymbol
	map[token.Type]string{
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxSymbols : SyntaxSymbol ∙
	map[token.Type]string{
		token.Type5:  ";",
		token.Type26: "|",
	},
	// SyntaxSymbols : ∙SyntaxSymbol SyntaxSymbols
	map[token.Type]string{
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxSymbols : SyntaxSymbol ∙SyntaxSymbols
	map[token.Type]string{
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
	},
	// SyntaxSymbols : SyntaxSymbol SyntaxSymbols ∙
	map[token.Type]string{
		token.Type5:  ";",
		token.Type26: "|",
	},
	// TokID : ∙tokid
	map[token.Type]string{
		token.Type23: "tokid",
	},
	// TokID : tokid ∙
	map[token.Type]string{
		token.Type4:  ":",
		token.Type5:  ";",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
		token.Type26: "|",
	},
	// UnicodeClass : ∙letter
	map[token.Type]string{
		token.Type16: "letter",
	},
	// UnicodeClass : letter ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// UnicodeClass : ∙upcase
	map[token.Type]string{
		token.Type24: "upcase",
	},
	// UnicodeClass : upcase ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// UnicodeClass : ∙lowcase
	map[token.Type]string{
		token.Type17: "lowcase",
	},
	// UnicodeClass : lowcase ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// UnicodeClass : ∙number
	map[token.Type]string{
		token.Type20: "number",
	},
	// UnicodeClass : number ∙
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
}

var followSets = []map[token.Type]string{
	// GoGLL
	map[token.Type]string{
		token.EOF: "EOF",
	},
	// LexAlternates
	map[token.Type]string{
		token.Type2:  ")",
		token.Type7:  ">",
		token.Type11: "]",
		token.Type27: "}",
	},
	// LexBracket
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexGroup
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexOneOrMore
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexOptional
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexRule
	map[token.Type]string{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// LexSymbol
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// LexZeroOrMore
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
	// NT
	map[token.Type]string{
		token.Type4:  ":",
		token.Type5:  ";",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
		token.Type26: "|",
	},
	// Package
	map[token.Type]string{
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// RegExp
	map[token.Type]string{
		token.Type2:  ")",
		token.Type5:  ";",
		token.Type7:  ">",
		token.Type11: "]",
		token.Type26: "|",
		token.Type27: "}",
	},
	// Rule
	map[token.Type]string{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// Rules
	map[token.Type]string{
		token.EOF: "EOF",
	},
	// SyntaxAlternate
	map[token.Type]string{
		token.Type5:  ";",
		token.Type26: "|",
	},
	// SyntaxAlternates
	map[token.Type]string{
		token.Type5: ";",
	},
	// SyntaxRule
	map[token.Type]string{
		token.EOF:    "EOF",
		token.Type19: "nt",
		token.Type23: "tokid",
	},
	// SyntaxSymbol
	map[token.Type]string{
		token.Type5:  ";",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
		token.Type26: "|",
	},
	// SyntaxSymbols
	map[token.Type]string{
		token.Type5:  ";",
		token.Type26: "|",
	},
	// TokID
	map[token.Type]string{
		token.Type4:  ":",
		token.Type5:  ";",
		token.Type19: "nt",
		token.Type22: "string_lit",
		token.Type23: "tokid",
		token.Type26: "|",
	},
	// UnicodeClass
	map[token.Type]string{
		token.Type1:  "(",
		token.Type2:  ")",
		token.Type3:  ".",
		token.Type5:  ";",
		token.Type6:  "<",
		token.Type7:  ">",
		token.Type8:  "[",
		token.Type11: "]",
		token.Type13: "any",
		token.Type14: "char_lit",
		token.Type16: "letter",
		token.Type17: "lowcase",
		token.Type18: "not",
		token.Type20: "number",
		token.Type24: "upcase",
		token.Type25: "{",
		token.Type26: "|",
		token.Type27: "}",
	},
}

/*** Errors ***/

type Error struct {
	cI           int
	Slot         slot.Label
	Token        *token.Token
	Line, Column int
	Expected     map[token.Type]string
}

func (pe *Error) String() string {
	w := new(bytes.Buffer)
	fmt.Fprintf(w, "Parse Error: %s I[%d]=%s at line %d col %d\n",
		pe.Slot, pe.cI, pe.Token, pe.Line, pe.Column)
	exp := []string{}
	for _, e := range pe.Expected {
		exp = append(exp, e)
	}
	fmt.Fprintf(w, "Expected one of: [%s]", strings.Join(exp, ","))
	return w.String()
}

func parseError(slot slot.Label, i int, expected map[token.Type]string) {
	pe := &Error{cI: i, Slot: slot, Token: lex.Tokens[i], Expected: expected}
	parseErrors = append(parseErrors, pe)
}

func sortParseErrors() {
	sort.Slice(parseErrors,
		func(i, j int) bool {
			return parseErrors[j].Token.Lext < parseErrors[i].Token.Lext
		})
	for _, pe := range parseErrors {
		pe.Line, pe.Column = lex.GetLineColumn(pe.Token.Lext)
	}
}

func parseErrorError(err error) {
	fmt.Printf("Error: %s\n", err)
	os.Exit(1)
}

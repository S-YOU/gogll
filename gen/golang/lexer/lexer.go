package lexer

import (
	"bytes"
	"path/filepath"
	"text/template"

	"github.com/goccmack/gogll/ast"
	"github.com/goccmack/goutil/ioutil"
)

type Data struct {
	Package string
}

func Gen(lexDir string, g *ast.GoGLL) {
	tmpl, err := template.New("lexer").Parse(tmplSrc)
	if err != nil {
		panic(err)
	}
	buf := new(bytes.Buffer)
	if err = tmpl.Execute(buf, getData(g)); err != nil {
		panic(err)
	}
	if err = ioutil.WriteFile(filepath.Join(lexDir, "lexer.go"), buf.Bytes()); err != nil {
		panic(err)
	}
}

func getData(g *ast.GoGLL) *Data {
	return &Data{
		Package: g.Package.GetString(),
	}
}

const tmplSrc = `
// Package lexer is generated by GoGLL. Do not edit.
package lexer

import(
    "io/ioutil"
    "strings"
	"unicode"

    "github.com/goccmack/goutil/md"

    "{{.Package}}/token"
)

type Lexer struct {
    I []rune
    Tokens []*token.Token
    pos int
}

func NewFile(fname string) *Lexer {
    if strings.HasSuffix(fname, ".md") {
        src, err := md.GetSource(fname)
        if err != nil {
            panic(err)
        }
        return New([]rune(src))
    }
    buf, err := ioutil.ReadFile(fname)
    if err != nil {
        panic(err)
    }
    return New([]rune(string(buf)))
}

// GetLineColumn returns the line and column of rune[i] in the input
func (l *Lexer) GetLineColumn(i int) (line, col int) {
	line, col = 1, 1
	for j := 0; j < i; j++ {
		switch l.I[j] {
		case '\n':
			line++
			col = 1
		case '\t':
			col += 4
		default:
			col++
		}
	}
	return
}

func (l *Lexer) GetLineColumnOfToken(i int) (line, col int) {
    return l.GetLineColumn(l.Tokens[i].Lext)
}

// GetString returns the input string from the left extent of Token[lext] to
// the right extent of Token[rext]
func (l *Lexer) GetString(lext, rext int) string {
    return string(l.I[l.Tokens[lext].Lext:l.Tokens[rext].Rext])
}

func New(input []rune) *Lexer {
	lex := &Lexer{
		I:      input,
		Tokens: make([]*token.Token, 0, 2048),
		pos:    0,
	}
	lex.scan()
	lex.add(token.EOF, len(input), len(input))
	return lex
}

func (l *Lexer) add(t token.Type, lext, rext int) {
	l.Tokens = append(l.Tokens,
		token.New(t, lext, rext, l.I[lext:rext]))
}

func (l *Lexer) scan() {
	for l.pos < len(l.I) {
		l.skipWhiteSpace()
		if l.pos >= len(l.I) {
			return
		}

		lext := l.pos
		l.pos++
		switch l.I[lext] {
		case ':':
			l.add(token.Type0, lext, l.pos)
		case ';':
			l.add(token.Type1, lext, l.pos)
		case '|':
			l.add(token.Type7, lext, l.pos)
		case '"':
			l.scanStringLiteral(lext)
		default:
			switch {
			case unicode.IsLower(l.I[lext]):
				l.scanTokIDOrReserevedWord(lext)
			case unicode.IsUpper((l.I[lext])):
				l.scanNT(lext)
			default:
				l.add(token.Error, lext, l.pos)
			}
		}
	}
}

func (l *Lexer) scanNT(lext int) {
	for l.isIDChar() {
		l.pos++
	}
	l.add(token.Type3, lext, l.pos)
}

func (l *Lexer) scanTokIDOrReserevedWord(lext int) {
	for l.isIDChar() {
		l.pos++
	}
	switch string(l.I[lext:l.pos]) {
	case "empty":
		l.add(token.Type2, lext, l.pos)
	case "package":
		l.add(token.Type4, lext, l.pos)
	default:
		l.add(token.Type6, lext, l.pos)
	}
}

func (l *Lexer) isIDChar() bool {
	if l.pos >= len(l.I) {
		return false
	}
	c := l.I[l.pos]
	return unicode.IsLetter(c) || unicode.IsNumber(c) || c == '_'
}

func (l *Lexer) scanStringLiteral(lext int) {
	for l.pos < len(l.I) && l.I[l.pos] != '"' {
		l.pos++
	}
	if l.pos >= len(l.I) || l.I[l.pos] != '"' {
		l.add(token.Error, lext, l.pos)
	} else {
		l.pos++
		l.add(token.Type5, lext, l.pos)
	}
}

func (l *Lexer) skipWhiteSpace() {
	for l.pos < len(l.I) && unicode.IsSpace(l.I[l.pos]) {
		l.pos++
	}
}
`

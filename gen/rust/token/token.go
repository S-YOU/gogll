//  Copyright 2020 Marius Ackerman
//
//  Licensed under the Apache License, Version 2.0 (the "License");
//  you may not use this file except in compliance with the License.
//  You may obtain a copy of the License at
//
//    http://www.apache.org/licenses/LICENSE-2.0
//
//  Unless required by applicable law or agreed to in writing, software
//  distributed under the License is distributed on an "AS IS" BASIS,
//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//  See the License for the specific language governing permissions and
//  limitations under the License.

// Package token generates the Rust token module
package token

import (
	"bytes"
	"text/template"

	"github.com/goccmack/goutil/ioutil"

	"github.com/goccmack/gogll/im/tokens"
)

type Data struct {
	Types        []*TypeDef
	TypeToString []string
}

type TypeDef struct {
	Name, Comment string
}

func Gen(fname string, ts *tokens.Tokens) {
	// fmt.Println(fname)
	tmpl, err := template.New("Rust token").Parse(tmplSrc)
	if err != nil {
		panic(err)
	}
	w := new(bytes.Buffer)
	if err := tmpl.Execute(w, getData(ts)); err != nil {
		panic(err)
	}
	if err := ioutil.WriteFile(fname, w.Bytes()); err != nil {
		panic(err)
	}
}

func getData(ts *tokens.Tokens) *Data {
	return &Data{
		Types:        getTypes(ts),
		TypeToString: ts.TypeToString,
	}
}

func getTypes(ts *tokens.Tokens) (types []*TypeDef) {
	for i := range ts.TypeToString {
		types = append(types,
			&TypeDef{
				Name:    ts.TypeToString[i],
				Comment: ts.TypeToLiteral[i],
			})
	}
	return
}

const tmplSrc = `
//! Module token is generated by GoGLL. Do not edit

extern crate lazy_static;

use std::rc::Rc;
use std::fmt;
use lazy_static::lazy_static;
use std::collections::HashMap;

/// Token is returned by the lexer for every scanned lexical token
pub struct Token {
	pub typ: Type,
	pub lext: usize, 
	pub rext: usize,
	input: Rc<Vec<char>>,
}

#[derive(PartialEq, Eq, Hash, Clone, Copy)]
pub enum Type {	{{range $t := .Types}}
	{{$t.Name}}, // "{{$t.Comment}}"{{end}}
}

/**
New returns a new token.  
lext is the left extent and rext the right extent of the token in the input.  
input is the input slice scanned by the lexer.
*/
pub fn new<'a>(t: Type, lext: usize, rext: usize, input: &Rc<Vec<char>>) -> Rc<Token> {
	Rc::new(Token{
		typ:   t,
		lext:  lext,
		rext:  rext,
		input: input.clone(),
	})
}

impl Token {
	/// get_line_column returns the (line, column) of the left extent of the token
	pub fn get_line_column(&self) -> (usize, usize) {
		let mut line = 1;
		let mut col = 1;
		let mut j = 0;
		while j < self.lext {
			match self.input[j] {
			'\n' => {
				line += 1;
				col = 1
			},
			'\t' => col += 4,
			_ => col += 1
			}
			j += 1
		}
		(line, col)
	}
	
	/// get_input returns the input from which t was parsed.
	pub fn get_input(&self) -> Rc<Vec<char>> {
		Rc::clone(&self.input)
	}
	
	/// literal returns the literal runes of t scanned by the lexer
	pub fn literal(&self) -> Vec<char> {
		self.input[self.lext..self.rext].to_vec()
	}
	
}

impl <'a>fmt::Display for Token {
	fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
		let (ln, col) = self.get_line_column();
		write!(f, "({}, ({},{}) {})", 
			self.typ, ln, col, self.literal().iter().collect::<String>())
	}

}

impl <'a>Type {
	/// id returns the token type ID of token Type t
	pub fn id(&self) -> &'a str {
		TYPE_TO_STRING[self]
	}
	
}

impl <'a>fmt::Display for Type {
	fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
		write!(f, "{}", TYPE_TO_STRING[self])
	}

}

lazy_static! {
    static ref TYPE_TO_STRING: HashMap<Type, &'static str> = {
        let mut m = HashMap::new(); {{range $t := .Types}}
		m.insert(Type::{{$t.Name}}, "{{$t.Comment}}");{{end}}
        m
    };
}

lazy_static! {
	static ref STRING_TO_TYPE: HashMap<&'static str, Type> = {
		let mut m = HashMap::new(); {{range $typ := .TypeToString}}
		m.insert("{{$typ}}", Type::{{$typ}}); {{end}}
		m
	};
}
`
